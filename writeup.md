# **Vehicle Detection**
## Writeup for Project 5


---

**Vehicle Detection Project**

The goals / steps of this project are the following:

* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier
* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. 
* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.
* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.
* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.
* Estimate a bounding box for vehicles detected.

[//]: # (Image References)
[image1]: ./examples/car_not_car.png
[image2]: ./examples/HOG_example.jpg
[image3]: ./examples/hot_windows.PNG
[image4]: ./examples/detection_pipeline.PNG
[video1]: ./project_video_output.mp4

## [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Vehicle-Detection/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  

You're reading it! This writeup is modified from the provided template and the majority of the code in the Jupyter notebook (`vehicle_detection.ipynb`) is taken directly from the Udacity Self-Driving Car Nanodegree lessons on Vehicle Detection.

### Histogram of Oriented Gradients (HOG)

#### 1. Explain how (and identify where in your code) you extracted HOG features from the training images.

The code for extracting HOG features is contained in the first code cell of the Jupyter notebook in the `get_hog_features` function, which is taken directly from one of the Udacity quiz solutions. It uses the `hog` function from `skimage`.

The `extract_features`, `single_img_features` and `find_cars` functions (again from the quiz solutions) use this function to extract hog features. The image is first converted to the chosen color space (`YUV`) and the hog features are extracted for either a single channel or all three.

I started by reading in all the `vehicle` and `non-vehicle` images.  Here is an example of one of each of the `vehicle` and `non-vehicle` classes:

![alt text][image1]

I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.

Here is an example using the `YUV` color space and HOG parameters of `orientations=8`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`, which was my final choice:

![alt text][image2]
(Note: this was the example image from the writeup template and was not generated by me, but does represent my final paramter selections.)

#### 2. Explain how you settled on your final choice of HOG parameters.

I tried various combinations of parameters, using the settings from the quiz solutions as the initial values. I focused most of my effort on selecting the number of orientations since I felt like the default settings for `pixels_per_cell` and `cells_per_block` did a good job of representing the outline of the car. I was also able to achieve reasonable classification performance without adjusting those two parameters.

I mostly toggled between 8 and 12 orientations to see which resulted in the best trade-off between classification accuracy and speed. I found that the higher number of orientations was not only slower, but also sometimes resulted in poorer accuracy with higher false positives. I did not spend much time on trying fewer orientations because I felt like HOG features were most critical to the classification and did not want to have substantially fewer HOG features than color or spatial features.

I settled on the YUV color space using channel 1 only because I felt like the grayscale gradients would best represent the outline of the car. I also tried other color spaces, like the saturation channel from HSV and HLS, for the HOG features, but found that the variation in car color and the high saturation of road features, like lines, caused the classifier performance to reduce. I did not choose to use all 3 channels due to speed and performance reasons.

#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).

I trained a SVM with an `rbf` kernel using HOG features, 12x12 spatial features, and a 32-bin color histogram in the `YUV` color space. These features were loaded into a single feature vector and normalized using the sklearn `StandardScaler`. The implementation of this can be found in the 3rd block of the "Build and Train Classifier" section of the Jupyter notebook and is largely taken from the quiz solutions.

For training data, I loaded a similar number of "car" and "not car" images, extracted their features, normalized those features, and shuffled them with sklearn's `train_test_split` using a 20% test size. The full data set was over 17,000 images, so I took only the first quarter of these for training to improve the speed of the SVM training and prediction.

### Sliding Window Search

#### 1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?

I decided to search over three overlapping sections of the image, using three different scales. These scales were 1.0 at the horizon, 1.5 in the middle, and 2.0 over the largest sub-image. All three sections started at the horizon, but ended at different locations depending on their size. The intent was to try to match multiple car sizes based on perspective, while not searching small windows close to the car. I had tried multiple scale sets, but settled on these to balance performance and speed of execution.

I modified the `find_cars` function from the lessons to implement this search. This function would take an already color converted sub-image (to improve speed), scale the sub-image so that each window would match the 64x64 training data size, and calculated HOG features once for the whole sub-image. It then looped through the windows in the x and y directions and extracted spatial, color_histogram, and hog_features from each one. The features were then normalized based on the training data and run through the SVM. If the SVM predicted the window had a car, then the window was added to the `hot_windows` list.

An example of the `hot_windows` output from this sliding search is shown below:

![alt text][image3]

#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?

Ultimately I searched on three scales using Y-channel HOG features plus spatially binned YUV-color and histograms of YUV-color in the feature vector. I chose an `rbf` kernel over a `linear` one for the SVM to reduce the number of false positives, but at the expense of speed. After much adjustment of the length and content of the feature vectors, I achieved reasonable performance with most of the false detections being related to the oncoming traffic or shadows. 

To limit the impact of these false detections, I implemented a thresholded heatmap as recommended by the lesson. The threshold effectively required that the car would be detected at multiple scales, which helped filter shadows that were detected at only a single scale.  Here are some example images of the `hot_windows` output, the heatmaps, and the final bounding boxes:

![alt text][image4]
---

### Video Implementation

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)
Here's a [link to my video result](./project_video_output.mp4)


#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.

I recorded the positions of positive detections in each frame of the video.  From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected. The implementation for all of this was taken directly from the lesson.

I then took the heatmap from each frame and appended it to a list (`heat_list` in `process_video`) that I maintained at a length of 5. I then summed these last 5 maps together to create an integrated heatmap of the last 5 frames. I put this integrated heatmap through a threshold that was more than my original threshold times the length of `heat_list` times. This meant that multiple windows in the same location would need to show up repeatedly over 5 frames in order to be considered valid. This drastically filtered false detections.


---

### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  

The biggest issue that I faced was trying to eliminate false detections from shadows and from the lane barrier on the left of the video. In the beginning, I had been using a linear SVM as recommended in the lesson, but it resulted in many of these types of false positives, even with a large number of training images and after many attempts at tweaking the feature vector content. I could not consistently get the classifier to achieve much more than about 95% accuracy with the linear SVM, which would always result in false detections from shadows. 

When I moved to an `rbf` kernel for the SVM, the performance drastically improved. I was able to achieve >99% accuracy with optimal feature vector content and could consistently get around 98% when optimizing for speed. Unfortunately, this created another problem, which was that the pipeline was now too slow. Testing different parameters became painful due to the processing time. I improved the speed problem by cutting the training examples down by 75% and increasing the C-value of the SVM so that the `rbf` outcome would be less complex, but still more detailed than a linear solution.

I would say that this speed issue is the main weakness of this implementation. My pipeline would absolutely not work in the real world since the performance was at 2s per frame rather than 30 frames per second.

I think another weakness of this pipeline would be sensitivity to lighting conditions. Most of the video is well-lit and I have not tested the pipeline in poor light yet. The HOG features are based solely on the Y-channel, which is good for strong lighting, but will suffer if the contrast is reduced.

If I were to continue working on this, I would focus first on improving the speed. I don't like the use of HOG features and an SVM because they are slow and inaccurate. It seems like a CNN would be much faster and better at identifying cars. I would probably try to implement that first and then tweak it to work well in mutliple lighting conditions.

